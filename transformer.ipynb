{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "import torch\n",
    "import ipdb\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split \n",
    "from Formaters import base_format as bf\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "read_file = pd.read_csv (r'C:\\Users\\admitos\\Desktop\\household_power_consumption.txt')\n",
    "read_file.to_csv (r'C:\\Users\\admitos\\Desktop\\Î—ousehold_power_consumption.csv', index=None)\n",
    "\n",
    "aal_file = pd.read_csv(r'C:\\Users\\admitos\\Downloads\\AAL_data.csv')\n",
    "aap_file = pd.read_csv(r'C:\\Users\\admitos\\Downloads\\AAP_data.csv')\n",
    "aapl_file = pd.read_csv(r'C:\\Users\\admitos\\Downloads\\AAPL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aal_data = aal_file.loc[:, aal_file.columns != 'Name']\n",
    "aap_data = aap_file.loc[:, aap_file.columns != 'Name']\n",
    "aapl_data = aapl_file.loc[:, aapl_file.columns != 'Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_file['Date;Time;Global_active_power;Global_reactive_power;Voltage;Global_intensity;Sub_metering_1;Sub_metering_2;Sub_metering_3']\n",
    "read_file.loc[0]\n",
    "my_df = read_file['Date;Time;Global_active_power;Global_reactive_power;Voltage;Global_intensity;Sub_metering_1;Sub_metering_2;Sub_metering_3'].str.split(';', n=9, expand=True)\n",
    "#my_df = pd.read_csv(read_file, sep=\"[;]\", engine='python', skiprows=1, names=['Date', 'Time', 'Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3'])\n",
    "my_df.columns = ['Date', 'Time', 'Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 1 (Temporal Fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_scalers(df):\n",
    "    \"\"\"Calibrates scalers using the data supplied.\n",
    "    Args:\n",
    "      df: Data to use to calibrate scalers.\n",
    "    \"\"\"\n",
    "    print('Setting scalers with training data...')\n",
    "\n",
    "    _column_definition = [('id', bf.DataTypes.REAL_VALUED, bf.InputTypes.ID),\n",
    "                          ('hours_from_start', bf.DataTypes.REAL_VALUED, bf.InputTypes.TIME),\n",
    "                          ('power_usage', bf.DataTypes.REAL_VALUED, bf.InputTypes.TARGET),\n",
    "                          ('hour', bf.DataTypes.REAL_VALUED, bf.InputTypes.KNOWN_INPUT),\n",
    "                          ('day_of_week', bf.DataTypes.REAL_VALUED, bf.InputTypes.KNOWN_INPUT),\n",
    "                          ('hours_from_start', bf.DataTypes.REAL_VALUED, bf.InputTypes.KNOWN_INPUT),\n",
    "                          ('categorical_id', bf.DataTypes.CATEGORICAL, bf.InputTypes.STATIC_INPUT)]\n",
    "\n",
    "    column_definitions = self.get_column_definition()\n",
    "    id_column = utils.get_single_col_by_input_type(bf.InputTypes.ID, column_definitions)\n",
    "    target_column = utils.get_single_col_by_input_type(bf.InputTypes.TARGET, column_definitions)\n",
    "\n",
    "    # Format real scalers\n",
    "    real_inputs = utils.extract_cols_from_data_type(bf.DataTypes.REAL_VALUED, column_definitions, {bf.InputTypes.ID, bf.InputTypes.TIME})\n",
    "\n",
    "    # Initialise scaler caches\n",
    "    _real_scalers = {}\n",
    "    _target_scaler = {}\n",
    "    identifiers = []\n",
    "    for identifier, sliced in df.groupby(id_column):\n",
    "\n",
    "      if len(sliced) >= self._time_steps:\n",
    "\n",
    "        data = sliced[real_inputs].values\n",
    "        targets = sliced[[target_column]].values\n",
    "        _real_scalers[identifier] = sklearn.preprocessing.StandardScaler().fit(data)\n",
    "\n",
    "        _target_scaler[identifier] = sklearn.preprocessing.StandardScaler().fit(targets)\n",
    "      identifiers.append(identifier)\n",
    "\n",
    "    # Format categorical scalers\n",
    "    categorical_inputs = utils.extract_cols_from_data_type(bf.DataTypes.CATEGORICAL, column_definitions,{bf.InputTypes.ID, bf.InputTypes.TIME})\n",
    "\n",
    "    categorical_scalers = {}\n",
    "    num_classes = []\n",
    "    for col in categorical_inputs:\n",
    "      # Set all to str so that we don't have mixed integer/string columns\n",
    "      srs = df[col].apply(str)\n",
    "      categorical_scalers[col] = sklearn.preprocessing.LabelEncoder().fit(srs.values)\n",
    "      num_classes.append(srs.nunique())\n",
    "\n",
    "    # Set categorical scaler outputs\n",
    "    self._cat_scalers = categorical_scalers\n",
    "    self._num_classes_per_cat_input = num_classes\n",
    "\n",
    "    # Extract identifiers in case required\n",
    "    self.identifiers = identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inputs(df):\n",
    "    \"\"\"Performs feature transformations. This includes both feature engineering, preprocessing and normalisation.\n",
    "    Args:\n",
    "      df: Data frame to transform.\n",
    "    Returns:\n",
    "      Transformed data frame.\n",
    "    \"\"\"\n",
    "\n",
    "    if self._real_scalers is None and self._cat_scalers is None:\n",
    "      raise ValueError('Scalers have not been set!')\n",
    "\n",
    "    # Extract relevant columns\n",
    "    column_definitions = self.get_column_definition()\n",
    "    id_col = utils.get_single_col_by_input_type(bf.InputTypes.ID, column_definitions)\n",
    "    real_inputs = utils.extract_cols_from_data_type(bf.DataTypes.REAL_VALUED, column_definitions, {bf.InputTypes.ID, bf.InputTypes.TIME})\n",
    "    categorical_inputs = utils.extract_cols_from_data_type(bf.DataTypes.CATEGORICAL, column_definitions, {bf.InputTypes.ID, bf.InputTypes.TIME})\n",
    "\n",
    "    # Transform real inputs per entity\n",
    "    df_list = []\n",
    "    for identifier, sliced in df.groupby(id_col):\n",
    "      # Filter out any trajectories that are too short\n",
    "      if len(sliced) >= self._time_steps:\n",
    "        sliced_copy = sliced.copy()\n",
    "        sliced_copy[real_inputs] = self._real_scalers[identifier].transform(\n",
    "            sliced_copy[real_inputs].values)\n",
    "        df_list.append(sliced_copy)\n",
    "\n",
    "    output = pd.concat(df_list, axis=0)\n",
    "\n",
    "    # Format categorical inputs\n",
    "    for col in categorical_inputs:\n",
    "      string_df = df[col].apply(str)\n",
    "      output[col] = self._cat_scalers[col].transform(string_df)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting train-valid-test splits.\n",
      "1500000 \n",
      "\n",
      "100000 \n",
      "\n",
      "475259 \n",
      "\n",
      "Setting scalers with training data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DataTypes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21168\\908004915.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#return (transform_inputs(data) for data in [train, validation, test])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21168\\908004915.py\u001b[0m in \u001b[0;36msplit_data\u001b[1;34m(df, train_boundary, test_boundary)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mset_scalers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#return (transform_inputs(data) for data in [train, validation, test])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21168\\3060152882.py\u001b[0m in \u001b[0;36mset_scalers\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Setting scalers with training data...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     _column_definition = [('id', DataTypes.REAL_VALUED, InputTypes.ID),\n\u001b[0m\u001b[0;32m      9\u001b[0m                           \u001b[1;33m(\u001b[0m\u001b[1;34m'hours_from_start'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataTypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREAL_VALUED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInputTypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTIME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                           \u001b[1;33m(\u001b[0m\u001b[1;34m'power_usage'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataTypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREAL_VALUED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInputTypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataTypes' is not defined"
     ]
    }
   ],
   "source": [
    "def split_data(df, train_boundary=1500000, test_boundary=1600000):\n",
    "    \"\"\"Splits data frame into training-validation-test data frames.\n",
    "    This also calibrates scaling object, and transforms data for each split.\n",
    "    Args:\n",
    "      df: Source data frame to split.\n",
    "      valid_boundary: Starting index for validation data\n",
    "      test_boundary: Starting index for test data\n",
    "    Returns:\n",
    "      Tuple of transformed (train, valid, test) data.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Formatting train-valid-test splits.')\n",
    "\n",
    "    train = df.iloc[:train_boundary]\n",
    "    validation = df.iloc[train_boundary:test_boundary]\n",
    "    test = df.iloc[test_boundary:]\n",
    "\n",
    "    print(len(train), '\\n')\n",
    "    print(len(validation), '\\n')\n",
    "    print(len(test), '\\n')\n",
    "\n",
    "    set_scalers(train)\n",
    "    return(train,validation,test)\n",
    "    #return (transform_inputs(data) for data in [train, validation, test])\n",
    "\n",
    "train, validation, test = split_data(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileLoss(nn.Module):\n",
    "    ## From: https://medium.com/the-artificial-impostor/quantile-regression-part-2-6fdbc26b2629\n",
    "\n",
    "    def __init__(self, quantiles):\n",
    "        ##takes a list of quantiles\n",
    "        super().__init__()\n",
    "        self.quantiles = quantiles\n",
    "        \n",
    "    def forward(self, preds, target):\n",
    "        assert not target.requires_grad\n",
    "        assert preds.size(0) == target.size(0)\n",
    "        losses = []\n",
    "        for i, q in enumerate(self.quantiles):\n",
    "            errors = target - preds[:, i]\n",
    "            losses.append(torch.max((q-1)*errors, q*errors).unsqueeze(1))\n",
    "\n",
    "        loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    ## Takes any module and stacks the time dimension with the batch dimenison of inputs before apply the module\n",
    "    ## From: https://discuss.pytorch.org/t/any-pytorch-function-can-work-as-keras-timedistributed/1346/4\n",
    "    \n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    # Gated Linear Unit\n",
    "    def __init__(self, input_size):\n",
    "        super(GLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,input_size)\n",
    "        self.fc2 = nn.Linear(input_size, input_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        sig = self.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return(torch.mul(sig, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedResidualNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_state_size, output_size, dropout, hidden_context_size=None, batch_first=False):\n",
    "        super(GatedResidualNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_context_size = hidden_context_size\n",
    "        self.hidden_state_size=hidden_state_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        if self.input_size!=self.output_size:\n",
    "            self.skip_layer = TimeDistributed(nn.Linear(self.input_size, self.output_size))\n",
    "\n",
    "        self.fc1 = TimeDistributed(nn.Linear(self.input_size, self.hidden_state_size), batch_first=batch_first)\n",
    "        self.elu1 = nn.ELU()\n",
    "        \n",
    "        if self.hidden_context_size is not None:\n",
    "            self.context = TimeDistributed(nn.Linear(self.hidden_context_size, self.hidden_state_size),batch_first=batch_first)\n",
    "            \n",
    "        self.fc2 = TimeDistributed(nn.Linear(self.hidden_state_size,  self.output_size), batch_first=batch_first)\n",
    "        self.elu2 = nn.ELU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.bn = TimeDistributed(nn.BatchNorm1d(self.output_size),batch_first=batch_first)\n",
    "        self.gate = TimeDistributed(GLU(self.output_size), batch_first=batch_first)\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "\n",
    "        if self.input_size!=self.output_size:\n",
    "            residual = self.skip_layer(x)\n",
    "        else:\n",
    "            residual = x\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        if context is not None:\n",
    "            context = self.context(context)\n",
    "            x = x+context\n",
    "\n",
    "        x = self.elu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gate(x)\n",
    "        x = x+residual\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=160):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x * math.sqrt(self.d_model)\n",
    "            seq_len = x.size(0)\n",
    "            pe = self.pe[:, :seq_len].view(seq_len,1,self.d_model)\n",
    "            x = x + pe\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableSelectionNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_inputs, hidden_size, dropout, context=None):\n",
    "        super(VariableSelectionNetwork, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dropout = dropout\n",
    "        self.context = context\n",
    "\n",
    "        if self.context is not None:\n",
    "            self.flattened_grn = GatedResidualNetwork(self.num_inputs*self.input_size, self.hidden_size, self.num_inputs, self.dropout, self.context)\n",
    "        else:\n",
    "            self.flattened_grn = GatedResidualNetwork(self.num_inputs*self.input_size, self.hidden_size, self.num_inputs, self.dropout)\n",
    "\n",
    "        self.single_variable_grns = nn.ModuleList()\n",
    "        for i in range(self.num_inputs):\n",
    "            self.single_variable_grns.append(GatedResidualNetwork(self.input_size, self.hidden_size, self.hidden_size, self.dropout))\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, embedding, context=None):\n",
    "        if context is not None:\n",
    "            sparse_weights = self.flattened_grn(embedding, context)\n",
    "        else:\n",
    "            sparse_weights = self.flattened_grn(embedding)\n",
    "\n",
    "        sparse_weights = self.softmax(sparse_weights).unsqueeze(2)\n",
    "\n",
    "        var_outputs = []\n",
    "        for i in range(self.num_inputs):\n",
    "            ##select slice of embedding belonging to a single input\n",
    "            var_outputs.append(self.single_variable_grns[i](embedding[:,:, (i*self.input_size) : (i+1)*self.input_size]))\n",
    "\n",
    "        var_outputs = torch.stack(var_outputs, axis=-1)\n",
    "        outputs = var_outputs*sparse_weights\n",
    "        outputs = outputs.sum(axis=-1)\n",
    "\n",
    "        return outputs, sparse_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TFT, self).__init__()\n",
    "        self.device = config['device']\n",
    "        self.batch_size=config['batch_size']\n",
    "        self.static_variables = config['static_variables']\n",
    "        self.encode_length = config['encode_length']\n",
    "        self.time_varying_categoical_variables = config['time_varying_categoical_variables']\n",
    "        self.time_varying_real_variables_encoder = config['time_varying_real_variables_encoder']\n",
    "        self.time_varying_real_variables_decoder = config['time_varying_real_variables_decoder']\n",
    "        self.num_input_series_to_mask = config['num_masked_series']\n",
    "        self.hidden_size = config['lstm_hidden_dimension']\n",
    "        self.lstm_layers = config['lstm_layers']\n",
    "        self.dropout = config['dropout']\n",
    "        self.embedding_dim = config['embedding_dim']\n",
    "        self.attn_heads = config['attn_heads']\n",
    "        self.num_quantiles = config['num_quantiles']\n",
    "        self.valid_quantiles = config['vailid_quantiles']\n",
    "        self.seq_length = config['seq_length']\n",
    "        \n",
    "        self.static_embedding_layers = nn.ModuleList()\n",
    "        for i in range(self.static_variables):\n",
    "            emb = nn.Embedding(config['static_embedding_vocab_sizes'][i], config['embedding_dim']).to(self.device)\n",
    "            self.static_embedding_layers.append(emb)\n",
    "        \n",
    "        self.time_varying_embedding_layers = nn.ModuleList()\n",
    "        for i in range(self.time_varying_categoical_variables):\n",
    "            emb = TimeDistributed(nn.Embedding(config['time_varying_embedding_vocab_sizes'][i], config['embedding_dim']), batch_first=True).to(self.device)\n",
    "            self.time_varying_embedding_layers.append(emb)\n",
    "            \n",
    "        self.time_varying_linear_layers = nn.ModuleList()\n",
    "        for i in range(self.time_varying_real_variables_encoder):\n",
    "            emb = TimeDistributed(nn.Linear(1, config['embedding_dim']), batch_first=True).to(self.device)\n",
    "            self.time_varying_linear_layers.append(emb)\n",
    "\n",
    "        self.encoder_variable_selection = VariableSelectionNetwork(config['embedding_dim'],\n",
    "                                (config['time_varying_real_variables_encoder'] + config['time_varying_categoical_variables']),\n",
    "                                self.hidden_size, self.dropout, config['embedding_dim']*config['static_variables'])\n",
    "\n",
    "        self.decoder_variable_selection = VariableSelectionNetwork(config['embedding_dim'],\n",
    "                                (config['time_varying_real_variables_decoder'] + config['time_varying_categoical_variables']),\n",
    "                                self.hidden_size, self.dropout, config['embedding_dim']*config['static_variables'])\n",
    "\n",
    "        self.lstm_encoder_input_size = config['embedding_dim']*(config['time_varying_real_variables_encoder'] + config['time_varying_categoical_variables'] +\n",
    "                                                        config['static_variables'])\n",
    "        \n",
    "        self.lstm_decoder_input_size = config['embedding_dim']*(config['time_varying_real_variables_decoder'] + config['time_varying_categoical_variables'] +\n",
    "                                                        config['static_variables'])\n",
    "\n",
    "        self.lstm_encoder = nn.LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=config['dropout'])\n",
    "        \n",
    "        self.lstm_decoder = nn.LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.lstm_layers, dropout=config['dropout'])\n",
    "\n",
    "        self.post_lstm_gate = TimeDistributed(GLU(self.hidden_size))\n",
    "        self.post_lstm_norm = TimeDistributed(nn.BatchNorm1d(self.hidden_size))\n",
    "\n",
    "        self.static_enrichment = GatedResidualNetwork(self.hidden_size,self.hidden_size, self.hidden_size, self.dropout, config['embedding_dim']*self.static_variables)\n",
    "        \n",
    "        self.position_encoding = PositionalEncoder(self.hidden_size, self.seq_length)\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(self.hidden_size, self.attn_heads)\n",
    "        self.post_attn_gate = TimeDistributed(GLU(self.hidden_size))\n",
    "\n",
    "        self.post_attn_norm = TimeDistributed(nn.BatchNorm1d(self.hidden_size, self.hidden_size))\n",
    "        self.pos_wise_ff = GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, self.dropout)\n",
    "\n",
    "        self.pre_output_norm = TimeDistributed(nn.BatchNorm1d(self.hidden_size, self.hidden_size))\n",
    "        self.pre_output_gate = TimeDistributed(GLU(self.hidden_size))\n",
    "\n",
    "        self.output_layer = TimeDistributed(nn.Linear(self.hidden_size, self.num_quantiles), batch_first=True)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.lstm_layers, self.batch_size, self.hidden_size, device=self.device)\n",
    "        \n",
    "    def apply_embedding(self, x, static_embedding, apply_masking):\n",
    "        ###x should have dimensions (batch_size, timesteps, input_size)\n",
    "        ## Apply masking is used to mask variables that should not be accessed after the encoding steps\n",
    "        #Time-varying real embeddings \n",
    "        if apply_masking:\n",
    "            time_varying_real_vectors = []\n",
    "            for i in range(self.time_varying_real_variables_decoder):\n",
    "                emb = self.time_varying_linear_layers[i+self.num_input_series_to_mask](x[:,:,i+self.num_input_series_to_mask].view(x.size(0), -1, 1))\n",
    "                time_varying_real_vectors.append(emb)\n",
    "            time_varying_real_embedding = torch.cat(time_varying_real_vectors, dim=2)\n",
    "\n",
    "        else: \n",
    "            time_varying_real_vectors = []\n",
    "            for i in range(self.time_varying_real_variables_encoder):\n",
    "                emb = self.time_varying_linear_layers[i](x[:,:,i].view(x.size(0), -1, 1))\n",
    "                time_varying_real_vectors.append(emb)\n",
    "            time_varying_real_embedding = torch.cat(time_varying_real_vectors, dim=2)\n",
    "        \n",
    "         ##Time-varying categorical embeddings (ie hour)\n",
    "        time_varying_categoical_vectors = []\n",
    "        for i in range(self.time_varying_categoical_variables):\n",
    "            emb = self.time_varying_embedding_layers[i](x[:, :,self.time_varying_real_variables_encoder+i].view(x.size(0), -1, 1).long())\n",
    "            time_varying_categoical_vectors.append(emb)\n",
    "        time_varying_categoical_embedding = torch.cat(time_varying_categoical_vectors, dim=2)  \n",
    "\n",
    "        ##repeat static_embedding for all timesteps\n",
    "        static_embedding = torch.cat(time_varying_categoical_embedding.size(1)*[static_embedding])\n",
    "        static_embedding = static_embedding.view(time_varying_categoical_embedding.size(0),time_varying_categoical_embedding.size(1),-1 )\n",
    "        \n",
    "        ##concatenate all embeddings\n",
    "        embeddings = torch.cat([static_embedding, time_varying_categoical_embedding, time_varying_real_embedding], dim=2)\n",
    "        \n",
    "        return embeddings.view(-1,x.size(0),embeddings.size(2))\n",
    "    \n",
    "    def encode(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden()\n",
    "            \n",
    "        output, (hidden, cell) = self.lstm_encoder(x, (hidden, hidden))\n",
    "        return output, hidden\n",
    "        \n",
    "    def decode(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden()\n",
    "            \n",
    "        output, (hidden, cell) = self.lstm_decoder(x, (hidden,hidden))\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## inputs should be in this order\n",
    "        # static\n",
    "        # time_varying_categorical\n",
    "        # time_varying_real\n",
    "\n",
    "        embedding_vectors = []\n",
    "        for i in range(self.static_variables):\n",
    "            #only need static variable from the first timestep\n",
    "            emb = self.static_embedding_layers[i](x['identifier'][:,0, i].long().to(self.device))\n",
    "            embedding_vectors.append(emb)\n",
    "\n",
    "        ## Embedding and variable selection\n",
    "        static_embedding = torch.cat(embedding_vectors, dim=1)\n",
    "        embeddings_encoder = self.apply_embedding(x['inputs'][:,:self.encode_length,:].float().to(self.device), static_embedding, apply_masking=False)\n",
    "        embeddings_decoder = self.apply_embedding(x['inputs'][:,self.encode_length:,:].float().to(self.device), static_embedding, apply_masking=True)\n",
    "        embeddings_encoder, encoder_sparse_weights = self.encoder_variable_selection(embeddings_encoder[:,:,:-(self.embedding_dim*self.static_variables)],embeddings_encoder[:,:,-(self.embedding_dim*self.static_variables):])\n",
    "        embeddings_decoder, decoder_sparse_weights = self.decoder_variable_selection(embeddings_decoder[:,:,:-(self.embedding_dim*self.static_variables)],embeddings_decoder[:,:,-(self.embedding_dim*self.static_variables):])\n",
    " \n",
    "        pe = self.position_encoding(torch.zeros(self.seq_length, 1, embeddings_encoder.size(2)).to(self.device)).to(self.device)\n",
    "        \n",
    "        embeddings_encoder = embeddings_encoder + pe[:self.encode_length,:,:]\n",
    "        embeddings_decoder = embeddings_decoder + pe[self.encode_length:,:,:]\n",
    "\n",
    "        ## LSTM\n",
    "        lstm_input = torch.cat([embeddings_encoder,embeddings_decoder], dim=0)\n",
    "        encoder_output, hidden = self.encode(embeddings_encoder)\n",
    "        decoder_output, _ = self.decode(embeddings_decoder, hidden)\n",
    "        lstm_output = torch.cat([encoder_output, decoder_output], dim=0)\n",
    "\n",
    "        ##skip connection over lstm\n",
    "        lstm_output = self.post_lstm_gate(lstm_output+lstm_input)\n",
    "\n",
    "        ##static enrichment\n",
    "        static_embedding = torch.cat(lstm_output.size(0)*[static_embedding]).view(lstm_output.size(0), lstm_output.size(1), -1)   \n",
    "        attn_input = self.static_enrichment(lstm_output, static_embedding)\n",
    "\n",
    "        ##skip connection over lstm\n",
    "        attn_input = self.post_lstm_norm(lstm_output)\n",
    "        #attn_input = self.position_encoding(attn_input)\n",
    "\n",
    "        ##Attention\n",
    "        attn_output, attn_output_weights = self.multihead_attn(attn_input[self.encode_length:,:,:], attn_input[:self.encode_length,:,:], attn_input[:self.encode_length,:,:])\n",
    "\n",
    "        ##skip connection over attention\n",
    "        attn_output = self.post_attn_gate(attn_output) + attn_input[self.encode_length:,:,:]\n",
    "        attn_output = self.post_attn_norm(attn_output)\n",
    "\n",
    "        output = self.pos_wise_ff(attn_output) #[self.encode_length:,:,:])\n",
    "\n",
    "        ##skip connection over Decoder\n",
    "        output = self.pre_output_gate(output) + lstm_output[self.encode_length:,:,:]\n",
    "\n",
    "        #Final output layers\n",
    "        output = self.pre_output_norm(output)\n",
    "        output = self.output_layer(output.view(self.batch_size, -1, self.hidden_size))\n",
    "        \n",
    "        \n",
    "        return (output, encoder_output, decoder_output, attn_output, attn_output_weights, encoder_sparse_weights, decoder_sparse_weights)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements a transformer model that can be used for times series\n",
    "    forecasting. This time series transformer model is based on the paper by\n",
    "    Wu et al (2020) [1]. The paper will be referred to as \"the paper\".\n",
    "    A detailed description of the code can be found in my article here:\n",
    "    https://towardsdatascience.com/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e\n",
    "    In cases where the paper does not specify what value was used for a specific\n",
    "    configuration/hyperparameter, this class uses the values from Vaswani et al\n",
    "    (2017) [2] or from PyTorch source code.\n",
    "    Unlike the paper, this class assumes that input layers, positional encoding \n",
    "    layers and linear mapping layers are separate from the encoder and decoder, \n",
    "    i.e. the encoder and decoder only do what is depicted as their sub-layers \n",
    "    in the paper. For practical purposes, this assumption does not make a \n",
    "    difference - it merely means that the linear and positional encoding layers\n",
    "    are implemented inside the present class and not inside the \n",
    "    Encoder() and Decoder() classes.\n",
    "    [1] Wu, N., Green, B., Ben, X., O'banion, S. (2020). \n",
    "    'Deep Transformer Models for Time Series Forecasting: \n",
    "    The Influenza Prevalence Case'. \n",
    "    arXiv:2001.08317 [cs, stat] [Preprint]. \n",
    "    Available at: http://arxiv.org/abs/2001.08317 (Accessed: 9 March 2022).\n",
    "    [2] Vaswani, A. et al. (2017) \n",
    "    'Attention Is All You Need'.\n",
    "    arXiv:1706.03762 [cs] [Preprint]. \n",
    "    Available at: http://arxiv.org/abs/1706.03762 (Accessed: 9 March 2022).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, dec_seq_len: int, batch_first: bool, out_seq_len: int=58, dim_val: int=512, n_encoder_layers: int=4, n_heads: int=8,\n",
    "        n_decoder_layers: int=4, dropout_encoder: float=0.2, dropout_decoder: float=0.2, dropout_pos_enc: float=0.1, dim_feedforward_encoder: int=2048,\n",
    "        dim_feedforward_decoder: int=2048, num_predicted_features: int=1): \n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: int, number of input variables. 1 if univariate.\n",
    "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
    "            dim_val: int, aka d_model. All sub-layers in the model produce \n",
    "                     outputs of dimension dim_val\n",
    "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
    "            n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
    "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
    "            dropout_encoder: float, the dropout rate of the encoder\n",
    "            dropout_decoder: float, the dropout rate of the decoder\n",
    "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
    "            dim_feedforward_encoder: int, number of neurons in the linear layer \n",
    "                                     of the encoder\n",
    "            dim_feedforward_decoder: int, number of neurons in the linear layer \n",
    "                                     of the decoder\n",
    "            num_predicted_features: int, the number of features you want to predict.\n",
    "                                    Most of the time, this will be 1 because we're\n",
    "                                    only forecasting FCR-N prices in DK2, but in\n",
    "                                    we wanted to also predict FCR-D with the same\n",
    "                                    model, num_predicted_features should be 2.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        #print(\"input_size is: {}\".format(input_size))\n",
    "        #print(\"dim_val is: {}\".format(dim_val))\n",
    "\n",
    "        # Creating the three linear layers needed for the model\n",
    "        self.encoder_input_layer = nn.Linear(in_features=input_size, out_features=dim_val)\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(in_features=num_predicted_features, out_features=dim_val)  \n",
    "        \n",
    "        self.linear_mapping = nn.Linear(in_features=dim_val, out_features=num_predicted_features)\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = bf.PositionalEncoder(d_model=dim_val, dropout=dropout_pos_enc)\n",
    "\n",
    "        # The encoder layer used in the paper is identical to the one used by\n",
    "        # Vaswani et al (2017) on which the PyTorch module is based.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_val, nhead=n_heads, dim_feedforward=dim_feedforward_encoder, \n",
    "                                                    dropout=dropout_encoder, batch_first=batch_first)\n",
    "\n",
    "        # Stack the encoder layers in nn.TransformerDecoder\n",
    "        # It seems the option of passing a normalization instance is redundant\n",
    "        # in my case, because nn.TransformerEncoderLayer per default normalizes\n",
    "        # after each sub-layer\n",
    "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=n_encoder_layers, norm=None)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=dim_val, nhead=n_heads, dim_feedforward=dim_feedforward_decoder,\n",
    "                                                    dropout=dropout_decoder, batch_first=batch_first)\n",
    "\n",
    "        # Stack the decoder layers in nn.TransformerDecoder\n",
    "        # It seems the option of passing a normalization instance is redundant\n",
    "        # in my case, because nn.TransformerDecoderLayer per default normalizes\n",
    "        # after each sub-layer\n",
    "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=n_decoder_layers, norm=None)\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, tgt_mask: Tensor=None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape:\n",
    "        [target_sequence_length, batch_size, num_predicted_features]\n",
    "        \n",
    "        Args:\n",
    "            src: the encoder's output sequence. Shape: (S,E) for unbatched input, \n",
    "                 (S, N, E) if batch_first=False or (N, S, E) if \n",
    "                 batch_first=True, where S is the source sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            tgt: the sequence to the decoder. Shape: (T,E) for unbatched input, \n",
    "                 (T, N, E)(T,N,E) if batch_first=False or (N, T, E) if \n",
    "                 batch_first=True, where T is the target sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            src_mask: the mask for the src sequence to prevent the model from \n",
    "                      using data points from the target sequence\n",
    "            tgt_mask: the mask for the tgt sequence to prevent the model from\n",
    "                      using data points from the target sequence\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"From model.forward(): Size of src as given to forward(): {}\".format(src.size()))\n",
    "        #print(\"From model.forward(): tgt size = {}\".format(tgt.size()))\n",
    "\n",
    "        # Pass throguh the input layer right before the encoder\n",
    "        src = self.encoder_input_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after input layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through the positional encoding layer\n",
    "        src = self.positional_encoding_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after pos_enc layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through all the stacked encoder layers in the encoder\n",
    "        # Masking is only needed in the encoder if input sequences are padded\n",
    "        # which they are not in this time series use case, because all my\n",
    "        # input sequences are naturally of the same length. \n",
    "        # (https://github.com/huggingface/transformers/issues/4083)\n",
    "        src = self.encoder(src=src) # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "        #print(\"From model.forward(): Size of src after encoder: {}\".format(src.size()))\n",
    "\n",
    "        # Pass decoder input through decoder input layer\n",
    "        decoder_output = self.decoder_input_layer(tgt) # src shape: [target sequence length, batch_size, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of decoder_output after linear decoder layer: {}\".format(decoder_output.size()))\n",
    "\n",
    "        #if src_mask is not None:\n",
    "            #print(\"From model.forward(): Size of src_mask: {}\".format(src_mask.size()))\n",
    "        #if tgt_mask is not None:\n",
    "            #print(\"From model.forward(): Size of tgt_mask: {}\".format(tgt_mask.size()))\n",
    "\n",
    "        # Pass throguh decoder - output shape: [batch_size, target seq len, dim_val]\n",
    "        decoder_output = self.decoder(tgt=decoder_output, memory=src, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
    "\n",
    "        #print(\"From model.forward(): decoder_output shape after decoder: {}\".format(decoder_output.shape))\n",
    "\n",
    "        # Pass through linear mapping\n",
    "        decoder_output = self.linear_mapping(decoder_output) # shape [batch_size, target seq len]\n",
    "        #print(\"From model.forward(): decoder_output size after linear_mapping = {}\".format(decoder_output.size()))\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_src_trg(self, sequence: torch.Tensor, enc_seq_len: int, target_seq_len: int): # -> Tuple[torch.tensor, torch.tensor, torch.tensor]\n",
    "\n",
    "        \"\"\"\n",
    "        Generate the src (encoder input), trg (decoder input) and trg_y (the target)\n",
    "        sequences from a sequence. \n",
    "        Args:\n",
    "            sequence: tensor, a 1D tensor of length n where \n",
    "                    n = encoder input length + target sequence length  \n",
    "            enc_seq_len: int, the desired length of the input to the transformer encoder\n",
    "            target_seq_len: int, the desired length of the target sequence (the \n",
    "                            one against which the model output is compared)\n",
    "        Return: \n",
    "            src: tensor, 1D, used as input to the transformer model\n",
    "            trg: tensor, 1D, used as input to the transformer model\n",
    "            trg_y: tensor, 1D, the target sequence against which the model output\n",
    "                is compared when computing loss. \n",
    "        \n",
    "        \"\"\"\n",
    "        #print(\"Called dataset.TransformerDataset.get_src_trg\")\n",
    "        assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "        \n",
    "        #print(\"From data.TransformerDataset.get_src_trg: sequence shape: {}\".format(sequence.shape))\n",
    "\n",
    "        # encoder input\n",
    "        src = sequence[:enc_seq_len] \n",
    "        \n",
    "        # decoder input. As per the paper, it must have the same dimension as the \n",
    "        # target sequence, and it must contain the last value of src, and all\n",
    "        # values of trg_y except the last (i.e. it must be shifted right by 1)\n",
    "        trg = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg shape before slice: {}\".format(trg.shape))\n",
    "\n",
    "        trg = trg[:, 0]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg shape after slice: {}\".format(trg.shape))\n",
    "\n",
    "        if len(trg.shape) == 1:\n",
    "            trg = trg.unsqueeze(-1)\n",
    "\n",
    "            #print(\"From data.TransformerDataset.get_src_trg: trg shape after unsqueeze: {}\".format(trg.shape))\n",
    "\n",
    "        \n",
    "        assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
    "\n",
    "        # The target sequence against which the model output will be compared to compute loss\n",
    "        trg_y = sequence[-target_seq_len:]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg_y shape before slice: {}\".format(trg_y.shape))\n",
    "\n",
    "        # We only want trg_y to consist of the target variable not any potential exogenous variables\n",
    "        trg_y = trg_y[:, 0]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg_y shape after slice: {}\".format(trg_y.shape))\n",
    "\n",
    "        assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
    "\n",
    "        return (src, trg, trg_y.squeeze(-1)) # change size from [batch_size, target_seq_len, num_features] to [batch_size, target_seq_len] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d49c3f6d6dd49f9272b571d9fad348ab55b8c6c3f691520d74ed0af1f69c3dd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

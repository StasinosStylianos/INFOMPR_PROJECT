{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, Tensor\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from Helpers import TST_model as tst\n",
    "from Helpers import inference as infer \n",
    "from Helpers import other_classes as other\n",
    "from Helpers import utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "#import dataset as ds\n",
    "#import transformer_timeseries as tst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_file = pd.read_csv (r'C:\\Users\\admitos\\Desktop\\household_power_consumption.txt')\n",
    "#read_file.to_csv (r'C:\\Users\\admitos\\Desktop\\Î—ousehold_power_consumption.csv', index=None)\n",
    "\n",
    "aal_file = pd.read_csv(r'C:\\Users\\admitos\\Desktop\\Visual Studio Notebooks\\Time Series Transformer\\AAL_data.csv')\n",
    "aap_file = pd.read_csv(r'C:\\Users\\admitos\\Desktop\\Visual Studio Notebooks\\Time Series Transformer\\AAP_data.csv')\n",
    "aapl_file = pd.read_csv(r'C:\\Users\\admitos\\Desktop\\Visual Studio Notebooks\\Time Series Transformer\\AAPL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aal_data = aal_file.loc[:, aal_file.columns != 'Name']\n",
    "aap_data = aap_file.loc[:, aap_file.columns != 'Name']\n",
    "aapl_data = aapl_file.loc[:, aapl_file.columns != 'Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_file.loc[0]\n",
    "#my_df = read_file['Date;Time;Global_active_power;Global_reactive_power;Voltage;Global_intensity;Sub_metering_1;Sub_metering_2;Sub_metering_3'].str.split(';', n=9, expand=True)\n",
    "#my_df.columns = ['Date', 'Time', 'Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, input_size: int, dec_seq_len: int, batch_first: bool, out_seq_len: int=58, dim_val: int=512, n_encoder_layers: int=4, n_heads: int=8,\n",
    "        n_decoder_layers: int=4, dropout_encoder: float=0.2, dropout_decoder: float=0.2, dropout_pos_enc: float=0.1, dim_feedforward_encoder: int=2048,\n",
    "        dim_feedforward_decoder: int=2048, num_predicted_features: int=1): \n",
    "\n",
    "        x = 1\n",
    "\n",
    "#transformer_model = TimeSeriesTransformer(input_size, dec_seq_len, batch_first)\n",
    "# model = TimeSeriesTransformer(dim_val=dim_val, input_size=input_size, dec_seq_len=dec_seq_len, max_seq_len=max_seq_len, \n",
    "#        out_seq_len=output_sequence_length, n_decoder_layers=n_decoder_layers, n_encoder_layers=n_encoder_layers, n_heads=n_heads)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "test_size = 0.1\n",
    "validation_size = 0.1\n",
    "batch_size = 64\n",
    "target_col_name = [\"open\"]\n",
    "timestamp_col = \"timestamp\"\n",
    "\n",
    "# Only use data from this date and onwards\n",
    "cutoff_date = datetime.datetime(2017, 1, 1) \n",
    "\n",
    "# Read data\n",
    "#data = utils.read_data(timestamp_col_name=timestamp_col)\n",
    "data = aal_data\n",
    "\n",
    "# Define input variables \n",
    "exogenous_vars = [] # should contain strings. Each string must correspond to a column name\n",
    "input_variables = [target_col_name] + exogenous_vars\n",
    "target_idx = 0 # index position of target in batched trg_y\n",
    "\n",
    "## Model parameters \n",
    "dim_val = 256 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "n_heads = 4 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "n_decoder_layers = 2 # Number of times the decoder layer is stacked in the decoder\n",
    "n_encoder_layers = 2 # Number of times the encoder layer is stacked in the encoder\n",
    "######## input_size = len(data.axes[1]) # The number of input variables. 1 if univariate forecasting.\n",
    "dec_seq_len = 21 # length of input given to decoder. Can have any integer value.\n",
    "enc_seq_len = 42 # length of input given to encoder. Can have any integer value.\n",
    "output_sequence_length = 10 # Length of the target sequence, i.e. how many time steps should your forecast cover\n",
    "max_seq_len = enc_seq_len # What's the longest sequence the model will encounter? Used to make the positional encoder\n",
    "window_size = enc_seq_len + output_sequence_length # used to slice data into sub-sequences\n",
    "step_size = 1 # Step size, i.e. how many time steps does the moving window move at each step\n",
    "in_features_encoder_linear_layer = 1024\n",
    "in_features_decoder_linear_layer = 1024\n",
    "batch_first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples length:  1007\n",
      "Validation samples lenght:  126\n",
      "Test samples length:  126\n",
      "From get_src_trg: data size = torch.Size([1007, 1])\n",
      "From get_src_trg: data size = torch.Size([126, 1])\n",
      "src shape changed from: torch.Size([64, 42, 1]) to: torch.Size([42, 64, 1])\n",
      "trg shape changed from: torch.Size([64, 10, 1]) to: torch.Size([42, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "# Remove test data from dataset\n",
    "train_val_data = data[:-(round(len(data)*test_size))]\n",
    "\n",
    "# Divide the training data to training and validation data\n",
    "training_data = train_val_data[:-(round(len(data)*validation_size))]\n",
    "print(\"Training samples length: \", len(training_data))\n",
    "validation_data = train_val_data[-(round(len(data)*validation_size)):]\n",
    "print(\"Validation samples lenght: \", len(validation_data))\n",
    "\n",
    "# Add test data seperately \n",
    "test_data = data[-(round(len(data)*test_size)):]\n",
    "print(\"Test samples length: \", len(test_data))\n",
    "\n",
    "# Make list of (start_idx, end_idx) pairs that are used to slice the time series sequence into chunkc. \n",
    "# Should be training data indices only\n",
    "training_indices = utils.get_indices_entire_sequence(data=training_data, window_size=window_size, step_size=step_size)\n",
    "validation_indices = utils.get_indices_entire_sequence(data=validation_data, window_size=window_size, step_size=step_size)\n",
    "\n",
    "# Making instance of custom dataset class\n",
    "training_dataset = other.TransformerDataset(data=torch.tensor(training_data[target_col_name].values).float(),\n",
    "    indices=training_indices, enc_seq_len=enc_seq_len, dec_seq_len=dec_seq_len, target_seq_len=output_sequence_length)\n",
    "\n",
    "validation_dataset = other.TransformerDataset(data=torch.tensor(validation_data[target_col_name].values).float(),\n",
    "    indices=validation_indices, enc_seq_len=enc_seq_len, dec_seq_len=dec_seq_len, target_seq_len=output_sequence_length)\n",
    "\n",
    "# Making a training dataloader\n",
    "training_dataloader = DataLoader(training_dataset, batch_size)\n",
    "i, batch = next(enumerate(training_dataloader))\n",
    "src, trg, trg_y = batch\n",
    "\n",
    "# Making a validation dataloader\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size)\n",
    "i, batch = next(enumerate(validation_dataloader))\n",
    "src, _, trg_y = batch\n",
    "\n",
    "# Permute from shape [batch size, seq len, num features] to [seq len, batch size, num features]\n",
    "if batch_first == False:\n",
    "    shape_before = src.shape\n",
    "    src = src.permute(1,0,2)\n",
    "    print(\"src shape changed from: {} to: {}\".format(shape_before, src.shape))\n",
    "\n",
    "    shape_before = trg.shape\n",
    "    trg = trg.permute(1,0,2)\n",
    "    print(\"trg shape changed from: {} to: {}\".format(shape_before, src.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________Entering the transformer___________________________\n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "The prediciton has shape:  torch.Size([10, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "transformer_model = tst.TimeSeriesTransformer(input_size=len(input_variables), dec_seq_len=enc_seq_len, batch_first=batch_first, \n",
    "                                            out_seq_len=output_sequence_length, dim_val=dim_val, n_encoder_layers=n_encoder_layers, \n",
    "                                            n_heads=n_heads, n_decoder_layers=n_decoder_layers, dim_feedforward_encoder=in_features_encoder_linear_layer, \n",
    "                                            dim_feedforward_decoder=in_features_decoder_linear_layer, num_predicted_features=1)\n",
    "\n",
    "# Make src mask for the encoder with size:\n",
    "# [batch_size*n_heads, output_sequence_length, enc_seq_len]\n",
    "src_mask = utils.generate_square_subsequent_mask(dim1=output_sequence_length, dim2=enc_seq_len)\n",
    "\n",
    "# Make tgt mask for the decoder with size:\n",
    "# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "tgt_mask = utils.generate_square_subsequent_mask(dim1=output_sequence_length, dim2=output_sequence_length)\n",
    "\n",
    "# Perform one pass through the transformer\n",
    "print(\"________________________Entering the transformer___________________________\\n\")\n",
    "output = transformer_model(src=src, tgt=trg, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "prediction = infer.run_encoder_decoder_inference(model=transformer_model, src=src, forecast_window=output_sequence_length, batch_size=src.shape[1])\n",
    "print(\"The prediciton has shape: \", prediction.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Epoch: 0\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 1\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 2\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 3\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 4\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 5\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 6\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 7\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 8\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 9\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 10\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 11\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 12\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 13\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n",
      "********************Epoch: 14\n",
      "\n",
      "\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 64, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([64, 10])\n",
      "The size of the target_y is:  torch.Size([64, 10])\n",
      "================= Passing through new training batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 59, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 59, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 59, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 59, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 59, 1])\n",
      "\n",
      "\n",
      "The size of the transformer output (i.e. prediction) is:  torch.Size([10, 59, 1])\n",
      "The size of the transformer output after squeeze and reshape is:  torch.Size([59, 10])\n",
      "The size of the target_y is:  torch.Size([59, 10])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 64, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 64, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 64, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 64, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 64, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 64, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 64, 1])\n",
      "================ Passing through new validation batch =================== \n",
      "\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([2, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([2, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([2, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([3, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([3, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([3, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([4, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([4, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([4, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([5, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([5, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([5, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([6, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([6, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([6, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([7, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([7, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([7, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([8, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([8, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([8, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([9, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([9, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([9, 10, 1])\n",
      "From model.forward(): Size of src as given to forward(): torch.Size([42, 10, 1])\n",
      "From model.forward(): tgt size = torch.Size([10, 10, 1])\n",
      "------------------ Start the forward in the transformer----------------------- \n",
      "\n",
      "From model.forward(): Size of src after input layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of src after encoder: torch.Size([42, 10, 256])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([10, 10, 256])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([10, 10, 1])\n",
      "The final prediction from the inference is:  torch.Size([10, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "forecast_window = output_sequence_length # supposing you're forecasting 48 hours ahead\n",
    "#########enc_seq_len = 168 # supposing you want the model to base its forecasts on the previous 7 days of data\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters())\n",
    "criterion = torch.nn.MSELoss()\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# Iterate over all epochs\n",
    "for epoch in range(epochs):\n",
    "    print(\"********************Epoch: {}\".format(epoch))\n",
    "    print(\"\\n\")\n",
    "    batch_training_loss = 0\n",
    "    batch_validation_loss = 0\n",
    "    # Iterate over all (x,y) pairs in training dataloader\n",
    "    for i, (src, tgt, tgt_y) in enumerate(training_dataloader):\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        print(\"================= Passing through new training batch =================== \\n\")\n",
    "        optimizer.zero_grad()\n",
    "        # correct the dimensions of the batches formation\n",
    "        if batch_first == False:\n",
    "            src = src.permute(1, 0, 2)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "\n",
    "        # Generate masks\n",
    "        src_mask = utils.generate_square_subsequent_mask(dim1=forecast_window, dim2=enc_seq_len)\n",
    "        tgt_mask = utils.generate_square_subsequent_mask(dim1=forecast_window, dim2=forecast_window)\n",
    "\n",
    "        # Make forecasts\n",
    "        prediction = transformer_model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "        # Compute and backprop loss\n",
    "        print(\"\\n\")\n",
    "        print(\"The size of the transformer output (i.e. prediction) is: \", prediction.shape)\n",
    "        prediction = torch.squeeze(prediction,2)\n",
    "        prediction = torch.reshape(prediction,(prediction.shape[1], prediction.shape[0]))\n",
    "        print(\"The size of the transformer output after squeeze and reshape is: \", prediction.shape)\n",
    "        print(\"The size of the target_y is: \", tgt_y.shape)\n",
    "        train_loss = criterion(tgt_y, prediction)\n",
    "        batch_training_loss += train_loss.item()\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Take optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Devide the loss by the number of iterations for consistency\n",
    "    epoch_train_loss = batch_training_loss/(i+1)\n",
    "    train_loss_list.append(epoch_train_loss)\n",
    "    # continue\n",
    "\n",
    "    # Iterate over all (x,y) pairs in validation dataloader\n",
    "    transformer_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (src, _, tgt_y) in enumerate(validation_dataloader):\n",
    "            if batch_first == False:\n",
    "                src = src.permute(1, 0, 2)\n",
    "                #tgt = tgt.permute(1, 0, 2)\n",
    "\n",
    "            print(\"================ Passing through new validation batch =================== \\n\")\n",
    "\n",
    "            prediction = infer.run_encoder_decoder_inference(model=transformer_model, src=src, forecast_window=forecast_window, batch_size=src.shape[1])\n",
    "    \n",
    "            prediction = torch.squeeze(prediction,2)\n",
    "            prediction = torch.reshape(prediction,(prediction.shape[1], prediction.shape[0]))\n",
    "            \n",
    "            val_loss = criterion(tgt_y, prediction)\n",
    "            batch_validation_loss += val_loss.item()\n",
    "\n",
    "        # Devide the loss by the number of iterations for consistency\n",
    "        epoch_val_loss = batch_validation_loss/(i+1)\n",
    "        val_loss_list.append(epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(len(train_loss_list))\n",
    "print(len(val_loss_list))\n",
    "plt.title(\"Training and Validation Loss through epochs\")\n",
    "plt.plot(train_loss_list)\n",
    "plt.plot(val_loss_list)\n",
    "plt.xlabel(\"Epochs\", size=12)\n",
    "plt.ylabel(\"Loss\", size=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d49c3f6d6dd49f9272b571d9fad348ab55b8c6c3f691520d74ed0af1f69c3dd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
